<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-HPRNJ62YY6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-HPRNJ62YY6');
  </script>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Zeo Liu | Robotics Research</title>
  <link rel="stylesheet" href="style.css" />
  <meta name="description" content="Zixi (Zeo) Liu — Robotics researcher focused on tactile sensing and grasp stability.">
  <meta name="keywords" content="robotics, tactile sensing, grasp stability, AI, manipulation, human-to-robot skill transfer">
  <meta name="author" content="Zixi (Zeo) Liu">
  <link rel="canonical" href="https://zixiliu.github.io/zeo-website/" />
  <meta name="google-site-verification" content="8OeMYfp4CH0ZaGTZKpPz_AAh_KV9cE4RU6qisi2OOXs" />

</head>
<body>
  <header>
    <div class="container">
      <img src="images/zixiliu_ID.jpeg" alt="Zixi (Zeo) Liu" class="profile-photo" />
      <h1>Zixi (Zeo) Liu</h1>
      <p>Robotics Researcher | Tactile Sensing & Grasp Stability</p>
      <nav>
        <a href="#about">About</a>
        <!-- <a href="#work">Work</a> -->
        <a href="#projects">Projects</a>
        <a href="#contact">Contact</a>
      </nav>
    </div>
  </header>

  <main>
    <section id="about" class="container">
      <h2>About Me</h2>

      <p>
        I'm a robotics researcher focused on manipulation. Most recently, I was a postdoctoral researcher at Meta FAIR under <b>Professor Jitendra Malik</b>. I earned my Ph.D. in Engineering Sciences from Harvard University with <b>Professor Rob Howe</b>, where my thesis was titled <i>"Reliable Grasping with Tactile Sensing"</i>. I completed both my bachelor's and M.Eng. degrees in Electrical Engineering and Computer Science at MIT, where my master's thesis was advised by <b>Professor Hugh Herr</b> at the MIT Media Lab.
      </p>

      <p>
        My work focuses on bridging <b>full-stack robotic manipulation</b> and <b>human-to-robot learning</b>. During my PhD, I designed tactile sensors and a dexterous robotic hand, integrated full systems, and developed friction-theoretical and physics–ML hybrid models for grasping. Building on this, at Meta FAIR, I created a pipeline to transfer skills from human demonstrations to any robotic embodiment, enabling rapid skill transfer and large-scale dataset generation.
      </p>

      <p>
        Other <u>topics</u> I've worked on include: reinforcement learning, real2sim2real, human2robot transfer learning, robotic simulation, large-scale robot learning, tactile representation learning.
      </p>
    </section>
<!--
    <section id="work" class="container">
      <h2>Work</h2>
      <p>
        Currently, I explore how tactile feedback and learning-based models can improve robotic grasping reliability. 
        Previously, at Meta AI, I developed pipelines for skill transfer from human demonstrations to robotic embodiments.
      </p>
    </section> -->

    <section id="projects" class="container">
      <h2>Selected Projects</h2>

      <div class="project">
        <h3>Human-to-Robot Skill Transfer</h3>
        <img src="images/human2robot.png" alt="Human-to-Robot Skill Transfer" class="project-image"  style="width: 70%; height: auto;"/>

        <div class="project-info">
          <p>
            Developed scalable pipelines for transferring manipulation skills from human demonstrations to various robotic embodiments,
            enabling generalizable and efficient robot learning.
          </p>

          <ul>
          <li>Pan, C., Wang, C., Qi, H., <strong style="color: maroon;">Liu, Z.</strong> Bharadhwaj, H. Sharma, A., Wu, T., Shi, G., Malik, J, Hogan, F., 2025. (Title removed for double blind review).</a><em> In Review for ICRA 2026. </em></li>
          <li><strong style="color: maroon;">Liu, Z.</strong>, Pan, C., Wang, C., Wu, T., Hogan, F., Malik, J. Learning from human videos: stable functional grasp transfer via contact. <em>In Preparation for Submission.</em></li>
        </ul>


        </div>
      </div>

      <div class="project">
        <img src="images/representation_learning.png" alt="Representation Learning" class="project-image" style="width: 70%; height: auto;"/>
        <div class="project-info">
          <h3>Robot Tactile Representation Learning</h3>
          <ul>
          <li><a href="https://arxiv.org/abs/2506.14754">Higuera, C., Sharma, A., Fan, T., Bodduluri, C.K., Boots, B., Kaess, M., Lambeta, M., Wu, T., <strong style="color: maroon;">Liu, Z.</strong>, Hogan, F.R. and Mukadam, M., 2025. Tactile Beyond Pixels: Multisensory Touch Representations for Robot Manipulation. arXiv preprint arXiv:2506.14754.</a></li>
          <li><a href="https://arxiv.org/abs/2505.11420">Sharma, A., Higuera, C., Bodduluri, C.K., <strong style="color: maroon;">Liu, Z.</strong>, Fan, T., Hellebrekers, T., Lambeta, M., Boots, B., Kaess, M., Wu, T. and Hogan, F.R., 2025. Self-supervised perception for tactile skin covered dexterous hands. arXiv preprint arXiv:2505.11420.</a> </li>
        </ul>

        </div>
      </div>

      <div class="project">
        <h3>Physics-ML Hybrid Models for Grasping</h3>
        <img src="images/hybrid.png" alt="Hybrid"
     class="project-image" style="width: 90%; height: auto;" />
        <div class="project-info">

        <ul>
          <li>Evaluating the role of physics-based models in robotic grasping - useful in physics-ML hybrid models especially when data is limited</li>
          <li>Enhancing machine learning by using parameters from physical models. </li>
          <li>Faster training, reduced data requirements, interpretable models, and improved grasp stability for deployment in unstructured environments.</li>
        </ul>
        <em><strong style="color: maroon;">Liu, Z.</strong>, Bayle, A., Janson, L., and Howe, R.D. The role of physics models: when does hybrid models make sense. <em>In Preparation for Submission.</em>
        </div>
      </div>


    <div class="project" >
        <h3>Novel Stochastic Friction Model</h3>
        <img src="images/friction.png" alt="friction"
     class="project-image" style="width: 70%; height: auto;" />
        <div class="project-info">

        <ul>
          <li> Discovery: The coefficient of friction is a stochastic variable and varies strongly with contact force and velocity.</li>
          <li> Development of an accurate model of slipping as a continuous function. </li>
          <li> Implication: potentially more realistic physics engines for simulation and machine learning.</li>
        </ul>
        <p>
            <a href="https://ieeexplore.ieee.org/document/10173619"><strong style="color: maroon;">Liu, Z.</strong> and Howe, R.D., 2023. Beyond Coulomb: Stochastic friction models for practical grasping and manipulation. IEEE Robotics and Automation Letters, 8(8), pp.5140-5147.</a>
          </p>
        </div>
    </div>

    <div class="project">
      <h3>How to Leverage Tactile Sensing without Tactile Sensors</h3>
        <img src="images/RL.png" alt="RL"
     class="project-image" style="width: 70%; height: auto;" />
        <div class="project-info">

        <ul>
          <li>Using Reinforcement Learning to evaluate robot hands with various tactile resolutions.</li>
          <li>Discovery: training with sensor-rich rewards allows for reduced sensing.</li>
          <li>Implication: enabling low-cost, high-performance robots and transforming success rate vs. cost trade-offs.</li>
        </ul>
          <p>
            <a href="https://arxiv.org/abs/2109.11234">Koenig, A., <strong style="color: maroon;">Liu, Z.</strong>, Janson, L. and Howe, R., 2022, October. The role of tactile sensing in learning and deploying grasp refinement algorithms. In 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (pp. 7766-7772). IEEE.</a>
            (Note: mentored first author.)
          </p>
        </div>
    </div>


    <div class="project">
      <h3>Sensor & Hardware Developement</h3>
        <img src="images/hardware.png" alt="hardware"
     class="project-image" style="width: 70%; height: auto;" />
        <div class="project-info">
          <!-- <h3>Tactile Sensing for Grasp Stability</h3> -->
          <ul>
          <li>Designed, developed, and characterized tactile sensors with gold-standard instruments.</li>
          <li>Designed & developed underactuated 3-fingered robot hand with tactile sensors.</li>
          <li>Integrated with UR-5 robot arm and independent gold-standard grasp paramter tracking instruments.</li>
        </ul>
        </div>
    </div>

    <div class="project">

      <h3>Collision Localization & Force Prediction</h3>
      <img src="images/collision.png" alt="collision"
     class="project-image" style="width: 30%; height: auto;" />
        <div class="project-info">
        <ul>
          <li> Accurate estimation of the contact force and contact location of a disturbance load using machine learning.</li>
          <li> Implication: enabling recovery from grasp error for robots with tactile sensors.</li>
        </ul>
        <p>
            <strong style="color: maroon;">Liu, Z.*</strong>, Umbach, T.*, and Howe, R.D., 2025. (Title removed for double blind review).</a><em> In Review for ICRA 2026. </em>
          </p>
        </div>
    </div>


    </section>

    <section id="contact" class="container">
      <h2>Contact</h2>
      <p>Email: <a href="mailto:zixiliu@alum.mit.edu">zixiliu@alum.mit.edu</a></p>
      <p>GitHub: <a href="https://github.com/zixiliu" target="_blank">@zixiliu</a></p>
      <p>LinkedIn: <a href="https://linkedin.com/in/zixi-zeo-liu-169b2591" target="_blank">linkedin.com/in/zixi-zeo-liu-169b2591</a></p>
      <p>Google Scholar: <a href="https://scholar.google.com/citations?user=4eeVPBoAAAAJ&hl=en&oi=ao" target="_blank">https://scholar.google.com/citations?user=4eeVPBoAAAAJ&hl=en&oi=ao</a></p>
    </section>
  </main>

  <footer>
    <p>© 2025 Zeo Liu. All rights reserved.</p>
  </footer>
</body>
</html>


